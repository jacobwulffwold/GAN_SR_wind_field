# This is the basic config.  name is used to create a folder in the root/experiments directory
[DEFAULT]

name                    = q1k_smaller_weights_lr
model                   = wind_field_GAN_3D
use_tensorboard_logger  = True
scale                   = 4
# Remove the `= <num>` to use CPU 
gpu_id                  = 0
also_log_to_terminal    = True
# Set this to true to use the generator_load_path, discriminator_load_path, and state_load_path to load a model.
load_model_from_save    = False
# Progress bar
display_bar             = True


[ENV]
# Root dir of project.
; root_path = /Users/jacobww/GAN_SR_WIND_FIELD
root_path = /cluster/home/jawold/GAN_SR_wind_field
; root_path = /Users/jacobww/GAN_SR_WIND_FIELD

log_subpath  = /log
tensorboard_subpath = /tensorboard_log
runs_subpath = /runs
fixed_seed = 2001
# If this has a value, and load_model_from_save = True, then G is loaded from this.
generator_load_path
# If this has a value, and load_model_from_save = True, then D is loaded from this.
discriminator_load_path
# If this has a value,  load_model_from_save = True, and resume_training_from_save = True then training is resumed from this.
state_load_path 

[GAN]
include_pressure = True
include_z_channel = True
number_of_z_layers = 10
conv_mode = 3D
start_date = [2017, 8, 4]
end_date = [2020, 10, 25]
interpolate_z = False
use_D_feature_extractor_cost = True
include_above_ground_channel = False
enable_slicing = True
slice_size = 64


# There are three datasets: TRAIN, TEST, and VAL.
# TRAIN/VAL are used for --train
# TEST is used for --test and --use
[DATASETTRAIN]
# allowed modes are downsampler (creates LR during runtime), lr (for --use), and hrlr (for specifying your own LR/HR)
# mode = downsampler
num_workers = 4
#num_workers = 8 <-- Original!
batch_size  = 32
# Only 128 is supported. Other dimensions would require a change in the discriminator.
hr_img_size = 128
# just for info
name  = WholeDataSet
# if mode is lr or hrlr, then dataroot_lr must also be specified
# dataroot_hr = /home/eirikeve/Programming/Datasets/Flickr2K/HR_256
# data augmentation: gaussian noise on the LR images
# data_aug_gaussian_noise = False
# gaussian_stddev = 0.00
# data_aug_shuffle = True
data_aug_flip = True
# This option is not currently in use.
data_aug_rot = True

[DATASETTEST]
mode = hrlr
num_workers = 4
#num_workers = 8 <-- Original!
batch_size  = 32
hr_img_size = 128
name  = Test
#dataroot_hr
#dataroot_lr = /home/eirikeve/Programming/Datasets/Set5/HR
data_aug_gaussian_noise = False
gaussian_stddev = 0.00
data_aug_shuffle = False
data_aug_flip = True
data_aug_rot = True

[DATASETVAL]
# mode = downsampler
num_workers = 4
batch_size  = 32
hr_img_size = 128
name  = Validation
data_aug_rot = True
#dataroot_hr = /home/eirikeve/Programming/Datasets/BSDS100/HR
#data_aug_gaussian_noise = False
#gaussian_stddev = 0.00
#data_aug_shuffle = False
data_aug_flip = True


[GENERATOR]
norm_type           = 'l1'
act_type            = leakyrelu
layer_mode          = CNA
# base # of features extracted = # channels
num_features        = 128
terrain_number_of_features = 16
# number of residual in residual dense blocks, was 16
num_RRDB            = 16
# this is not currently used
num_RDB_convs       = 5
RDB_res_scaling     = 0.2
RRDB_res_scaling    = 0.2
# RGB = 3
in_num_ch           = 3
out_num_ch          = 3
RDB_growth_chan     = 32
hr_kern_size        = 5
weight_init_scale   = 0.1
# lff = local feature fusion layer of the RDB
lff_kern_size       = 1
use_mixed_precision = True
dropout_probability = 0.1
max_norm = 1.0

[DISCRIMINATOR]
norm_type       = batch
act_type        = leakyrelu
layer_mode      = CNA
num_features    = 32
in_num_ch       = 3
feat_kern_size  = 3
weight_init_scale   = 0.2
use_mixed_precision = True
dropout_probability = 0.0

[TRAINING]
# See [ENV]
resume_training_from_save = True
learning_rate_g = 1e-5
#1e-4
learning_rate_d = 1e-5
#1e-4
adam_weight_decay_g = 0
adam_weight_decay_d = 0
adam_beta1_g = 0.9
adam_beta1_d = 0.9

# LR is decayed by factor lr_gamma for each entry in multistep_lr_steps if multistep_lr = True
multistep_lr = True
# ESRGAN has 50k, 100k, 200k, 300k
multistep_lr_steps = [3000, 6000, 10000, 20000, 30000, 50000]
; [50000, 100000, 150000, 200000]
lr_gamma = 0.5
gan_type = relativisticavg
adversarial_loss_weight = 0.005
#5e-3
feature_D_loss_weight = 0.05

gradient_xy_loss_weight = 1.0
gradient_z_loss_weight = 0.1

divergence_loss_weight = 0.005
xy_divergence_loss_weight = 0.2

pixel_loss_weight = 0.2
pixel_criterion = l1

# How often D is updated relative to G. 
d_g_train_ratio = 1
#0.01
; feature_criterion = l1
# ESRGAN has 1.0
; feature_weight = 1.0
# add noise to the labels for D - with stddev 0.05   (DEFAULT = False) - Thomas
use_noisy_labels = False

use_one_sided_label_smoothing = True
# With: Flip labels for real and fake data for D.
flip_labels = False
# Instance noise is gaussian noise added on the input to D
# beginning at var 1, decreasing linearly to var 0 at training end.
use_instance_noise = True
# iterations to train for
niter  = 1000
#300000
#niter = 200000 <-- ORIGINAL
val_period = 100
#1000
# output images and models are saved
save_model_period  = 10000
#150000
# for fetching logs to the logfiles. Not important.
log_period = 5
#100
train_eval_test_ratio = 0.99
